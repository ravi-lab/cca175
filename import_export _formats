text file : 


Import : 

using sqoop : 

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-textfile \
--target-dir /user/raviteja7/test_sqoop_import_nocomp \
-m 1 

[raviteja7@gw03 ~]$ hadoop fs -ls test_sqoop_import_nocomp
Found 2 items
-rw-r--r--   3 raviteja7 hdfs          0 2018-04-11 20:57 test_sqoop_import_nocomp/_SUCCESS
-rw-r--r--   3 raviteja7 hdfs    2999944 2018-04-11 20:57 test_sqoop_import_nocomp/part-m-00000


sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-textfile \
--target-dir /user/raviteja7/test_sqoop_import_gzip \
-m 1 \
--compression-codec gzip 

[raviteja7@gw03 ~]$ hadoop fs -ls test_sqoop_import_gzip
Found 2 items
-rw-r--r--   3 raviteja7 hdfs          0 2018-04-11 21:01 test_sqoop_import_gzip/_SUCCESS
-rw-r--r--   3 raviteja7 hdfs     471106 2018-04-11 21:01 test_sqoop_import_gzip/part-m-00000.gz

sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-textfile \
--target-dir /user/raviteja7/test_sqoop_import_snappy \
-m 1 \
--compression-codec snappy

[raviteja7@gw03 ~]$ hadoop fs -ls test_sqoop_import_snappy
Found 2 items
-rw-r--r--   3 raviteja7 hdfs          0 2018-04-11 23:53 test_sqoop_import_snappy/_SUCCESS
-rw-r--r--   3 raviteja7 hdfs     878331 2018-04-11 23:53 test_sqoop_import_snappy/part-m-00000.snappy


Now read using spark-shell 
val a=sc.textFile("/user/raviteja7/test_sqoop_import_nocomp/part-m-00000");

read compressied file 

val b=sc.textFile("/user/raviteja7/test_sqoop_import_gzip/part-m-00000.gz");

val c=sc.textFile("/user/raviteja7/test_sqoop_import_snappy/part-m-00000.snappy");


Save as textFile 

 a.saveAsTextFile("/user/raviteja7/order_saveastext");
 [raviteja7@gw03 ~]$ hadoop fs -ls order_saveastext
Found 3 items
-rw-r--r--   3 raviteja7 hdfs          0 2018-04-12 00:01 order_saveastext/_SUCCESS
-rw-r--r--   3 raviteja7 hdfs    1499979 2018-04-12 00:01 order_saveastext/part-00000
-rw-r--r--   3 raviteja7 hdfs    1499965 2018-04-12 00:01 order_saveastext/part-00001

Save as text file with compression 

a.saveAsTextFile("/user/raviteja7/order_saveastextgzip",classOf[org.apache.hadoop.io.compress.GzipCodec])

[raviteja7@gw03 ~]$ hadoop fs -ls order_saveastextgzip
Found 3 items
-rw-r--r--   3 raviteja7 hdfs          0 2018-04-12 00:05 order_saveastextgzip/_SUCCESS
-rw-r--r--   3 raviteja7 hdfs     233422 2018-04-12 00:05 order_saveastextgzip/part-00000.gz
-rw-r--r--   3 raviteja7 hdfs     237961 2018-04-12 00:05 order_saveastextgzip/part-00001.gz

b.saveAsTextFile("/user/raviteja7/order_saveastextSnappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])

[raviteja7@gw03 ~]$ hadoop fs -ls order_saveastextSnappy
Found 2 items
-rw-r--r--   3 raviteja7 hdfs          0 2018-04-12 00:07 order_saveastextSnappy/_SUCCESS
-rw-r--r--   3 raviteja7 hdfs     878331 2018-04-12 00:07 order_saveastextSnappy/part-00000.snappy


Saving as sequence file 
For sequence file ,we need to have a key 

val seq=a.map(o=>(o.split(",")(0).toInt,o))
seq.take(10).foreach(println)
(1,1,2013-07-25 00:00:00.0,11599,CLOSED)
(2,2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT)
(3,3,2013-07-25 00:00:00.0,12111,COMPLETE)
(4,4,2013-07-25 00:00:00.0,8827,CLOSED)
(5,5,2013-07-25 00:00:00.0,11318,COMPLETE)
(6,6,2013-07-25 00:00:00.0,7130,COMPLETE)
(7,7,2013-07-25 00:00:00.0,4530,COMPLETE)
(8,8,2013-07-25 00:00:00.0,2911,PROCESSING)
(9,9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT)
(10,10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT)

saveAsSequencefile not working 
Using compression
textMap.saveAsSequenceFile("/user/cloudera/ReadDiffFileFormat/text/textoutput/sequenceout/compressed", Some(classOf[org.apache.hadoop.io.compress.BZip2Codec]))

JSON file : To convert text to JSON we need data frame 


first create a tuple of all fields otherwise whole record gets stored as a string then convert to dataframe 

val textDF= a.map(p=>(p.split(",")(0),p.split(",")(1),p.split(",")(2),p.split(",")(3))).toDF
textDF.toJSON.saveAsTextFile("/user/raviteja7/orders_JSON");

textDF.toJSON.saveAsTextFile("/user/raviteja7/orders_JSON_BZIP",classOf[org.apache.hadoop.io.compress.BZip2Codec]);

Using write 
textDF.write.json("/user/raviteja7/order_json_write")
sqlContext.setConf("spark.sql.json.compression.codec", "snappy") // this will not work 
textDF.write.json("/user/raviteja7/order_json_write_snappy")

To Parquet 

textDF.write.parquet("user/raviteja7/order_parq_gzip")

with compression 
sqlContext.setConf("spark.sql.parquet.compression.codec", "gzip")
textDF.write.parquet("user/raviteja7/order_parq")


val jsonfile=sqlContext.read.json("/user/raviteja7/orders_JSON/part-00000")
jsonfile.